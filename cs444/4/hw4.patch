--- ../hw4vm2/linux-yocto-3.14/mm/slob.c	2017-06-12 10:52:38.229448793 -0700
+++ linux-yocto-3.14/mm/slob.c	2017-06-12 13:06:23.295377632 -0700
@@ -67,6 +67,8 @@
 #include <linux/rcupdate.h>
 #include <linux/list.h>
 #include <linux/kmemleak.h>
+#include <linux/linkage.h>
+#include <linux/syscalls.h>
 
 #include <trace/events/kmem.h>
 
@@ -87,6 +89,13 @@
 typedef s32 slobidx_t;
 #endif
 
+// counters to keep track of memory being used and claimed
+static unsigned long mem_used = 0;
+static unsigned long mem_claimed = 0;
+
+// counter for every time slob_alloc is called
+static unsigned long counter = 0;
+
 struct slob_block {
 	slobidx_t units;
 };
@@ -217,48 +226,129 @@
 static void *slob_page_alloc(struct page *sp, size_t size, int align)
 {
 	slob_t *prev, *cur, *aligned = NULL;
-	int delta = 0, units = SLOB_UNITS(size);
+	slob_t *best_cur = NULL, *best_prev = NULL, *best_aligned = NULL;
+	int delta = 0, best_delta = 0, print = 0, units = SLOB_UNITS(size);
+
+	if (counter > 5000) {
+		print = 1;
+		printk(KERN_INFO "Request size: %u\n", units);
+		printk(KERN_INFO "Available spaces: ");
+	}
+	
+	slobidx_t avail;
+	slobidx_t best_frag = 0;
 
 	for (prev = NULL, cur = sp->freelist; ; prev = cur, cur = slob_next(cur)) {
-		slobidx_t avail = slob_units(cur);
+		avail = slob_units(cur);
 
 		if (align) {
 			aligned = (slob_t *)ALIGN((unsigned long)cur, align);
 			delta = aligned - cur;
 		}
-		if (avail >= units + delta) { /* room enough? */
-			slob_t *next;
+		
+		if (print) {
+			printk(KERN_INFO "[%u]", slob_units(cur));
+		}
+
+		// update best fit
+		if (avail >= units + delta && (best_cur == NULL || best_frag > avail - (units + delta))) { /* room enough? */
+			best_aligned = aligned;
+			best_prev = prev;
+			best_cur = cur;
+			best_delta = delta;
+			best_frag = avail - (units + delta);
 
-			if (delta) { /* need to fragment head to align? */
-				next = slob_next(cur);
-				set_slob(aligned, avail - delta, next);
-				set_slob(cur, delta, aligned);
-				prev = cur;
-				cur = aligned;
-				avail = slob_units(cur);
+			if (best_frag == 0) { // exact match found, no more searching
+				break;
 			}
+		}
+		if (slob_last(cur)) {
+			break;
+		}
+	}
+
+	// allocate the best fit page found
+	if (best_cur != NULL) {
+		if (print) {
+			printk(KERN_INFO "| Best fit: %u\n", slob_units(best_cur));
+		}
+		
+		slob_t *best_next = NULL;
+		avail = slob_units(best_cur);
+
+		if (best_delta) { /* need to fragment head to align? */
+			best_next = slob_next(best_cur);
+			set_slob(best_aligned, avail - best_delta, best_next);
+			set_slob(best_cur, best_delta, best_aligned);
+			best_prev = best_cur;
+			best_cur = best_aligned;
+			avail = slob_units(best_cur);
+		}
+
+		best_next = slob_next(best_cur);
+
+		if (avail == units) { /* exact fit? unlink. */
+			if (best_prev)
+				set_slob(best_prev, slob_units(best_prev), best_next);
+			else
+				sp->freelist = best_next;
+		} else { /* fragment */
+			if (best_prev)
+				set_slob(best_prev, slob_units(best_prev), best_cur + units);
+			else
+				sp->freelist = best_cur + units;
+			set_slob(best_cur + units, avail - units, best_next);
+		}
+
+		sp->units -= units;
+		if (!sp->units)
+			clear_slob_page_free(sp);
+
+		return best_cur;
+	} else {
+		return NULL;
+	}
+}
 
-			next = slob_next(cur);
-			if (avail == units) { /* exact fit? unlink. */
-				if (prev)
-					set_slob(prev, slob_units(prev), next);
-				else
-					sp->freelist = next;
-			} else { /* fragment */
-				if (prev)
-					set_slob(prev, slob_units(prev), cur + units);
-				else
-					sp->freelist = cur + units;
-				set_slob(cur + units, avail - units, next);
+/*
+ * Find the best fit page for the memory being allocated. The best fit page 
+ * should have more than the requested amount of space and should have the
+ * least amount of left over space out of all the possible pages.
+ */
+static int slob_best_fit(struct page *sp, size_t size, int align)
+{
+	slob_t *prev, *cur, *aligned = NULL;
+	slob_t *best_cur = NULL;
+	int delta = 0, units = SLOB_UNITS(size);
+
+	slobidx_t best = -1;
+
+	// iterate through each slob
+	for (prev = NULL, cur = sp->freelist; ; prev = cur, cur = slob_next(cur)) {
+		// available space
+		slobidx_t avail = slob_units(cur);
+
+		if (align) {
+			aligned = (slob_t *)ALIGN((unsigned long)cur, align);
+			delta = aligned - cur;
+		}
+		// update best fit
+		if (avail >= units + delta && (best_cur == NULL || best > avail - (units + delta))) { /* room enough? */
+			best_cur = cur;
+			best = avail - (units + delta);
+			if (best == 0) {
+				return 0;
 			}
+		}
 
-			sp->units -= units;
-			if (!sp->units)
-				clear_slob_page_free(sp);
-			return cur;
+		// end loop once we reach last slob, return best block size for page
+		if (slob_last(cur)) {
+			if (best_cur != NULL) {
+				return best;
+			} else {
+				return -1;
+			}
 		}
-		if (slob_last(cur))
-			return NULL;
 	}
 }
 
@@ -268,11 +358,20 @@
 static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 {
 	struct page *sp;
-	struct list_head *prev;
 	struct list_head *slob_list;
 	slob_t *b = NULL;
 	unsigned long flags;
 
+	// increment and reset alloc counter
+	if (counter > 5000) {
+		counter = 0;
+	}
+	++counter;
+
+	// for finding page with best fit
+	struct page *best_sp = NULL;
+	int best_fit = -1;
+
 	if (size < SLOB_BREAK1)
 		slob_list = &free_slob_small;
 	else if (size < SLOB_BREAK2)
@@ -281,8 +380,12 @@
 		slob_list = &free_slob_large;
 
 	spin_lock_irqsave(&slob_lock, flags);
+
 	/* Iterate through each partially free page, try to find room */
 	list_for_each_entry(sp, slob_list, list) {
+		// reset current fit size
+		int cur_fit = -1;
+
 #ifdef CONFIG_NUMA
 		/*
 		 * If there's a node specification, search for a partial
@@ -294,21 +397,26 @@
 		/* Enough room on this page? */
 		if (sp->units < SLOB_UNITS(size))
 			continue;
+		
+		cur_fit = slob_best_fit(sp, size, align);
+		
+		if (cur_fit == 0) { // found page of exact fit
+			best_sp = sp;
+			break;
+		} else if (cur_fit > 0 && (best_fit == -1 || cur_fit < best_fit)) {
+			best_sp = sp;
+			best_fit = cur_fit;
+		}
+		continue;
+	}
 
-		/* Attempt to alloc */
-		prev = sp->list.prev;
-		b = slob_page_alloc(sp, size, align);
-		if (!b)
-			continue;
-
-		/* Improve fragment distribution and reduce our average
-		 * search time by starting our next search here. (see
-		 * Knuth vol 1, sec 2.5, pg 449) */
-		if (prev != slob_list->prev &&
-				slob_list->next != prev->next)
-			list_move_tail(slob_list, prev->next);
-		break;
+	if (best_fit >= 0) { // if page is found
+		if (best_sp != NULL) {
+			/* Attempt to alloc */
+			b = slob_page_alloc(best_sp, size, align);
+		}
 	}
+
 	spin_unlock_irqrestore(&slob_lock, flags);
 
 	/* Not enough space: must allocate a new page */
@@ -316,6 +424,10 @@
 		b = slob_new_pages(gfp & ~__GFP_ZERO, 0, node);
 		if (!b)
 			return NULL;
+		
+		// new allocated page added to total memory claimed
+		mem_claimed += PAGE_SIZE;
+		
 		sp = virt_to_page(b);
 		__SetPageSlab(sp);
 
@@ -331,6 +443,10 @@
 	}
 	if (unlikely((gfp & __GFP_ZERO) && b))
 		memset(b, 0, size);
+
+	// size of new allocated page added to total in use
+	mem_used += size;
+
 	return b;
 }
 
@@ -354,7 +470,13 @@
 
 	spin_lock_irqsave(&slob_lock, flags);
 
+	// memory freed from total in use
+	mem_used -= size;
+
 	if (sp->units + units == SLOB_UNITS(PAGE_SIZE)) {
+	// free a page and free up space in memory claimed
+		mem_claimed -= PAGE_SIZE;
+
 		/* Go directly to page allocator. Do not pass slob allocator */
 		if (slob_page_free(sp))
 			clear_slob_page_free(sp);
@@ -643,3 +765,12 @@
 {
 	slab_state = FULL;
 }
+
+asmlinkage unsigned long sys_get_mem_claimed(void) {
+	return mem_claimed;
+}
+
+asmlinkage unsigned long sys_get_mem_free(void) {
+	return mem_claimed - mem_used;
+}
+
--- ../hw4vm2/linux-yocto-3.14/arch/x86/syscalls/syscall_32.tbl	2017-06-07 19:47:58.640217796 -0700
+++ linux-yocto-3.14/arch/x86/syscalls/syscall_32.tbl	2017-06-12 12:51:45.815773641 -0700
@@ -359,3 +359,5 @@
 350	i386	finit_module		sys_finit_module
 351	i386	sched_setattr		sys_sched_setattr
 352	i386	sched_getattr		sys_sched_getattr
+353	i386	mem_claimed		sys_get_mem_claimed
+354	i386	mem_free		sys_get_mem_free
--- ../hw4vm2/linux-yocto-3.14/include/linux/syscalls.h	2017-06-07 19:48:01.897272976 -0700
+++ linux-yocto-3.14/include/linux/syscalls.h	2017-06-12 12:54:06.960121500 -0700
@@ -855,4 +855,6 @@
 asmlinkage long sys_kcmp(pid_t pid1, pid_t pid2, int type,
 			 unsigned long idx1, unsigned long idx2);
 asmlinkage long sys_finit_module(int fd, const char __user *uargs, int flags);
+asmlinkage unsigned long sys_get_mem_claimed(void);
+asmlinkage unsigned long sys_get_mem_free(void);
 #endif
